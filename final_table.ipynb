{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.log import lookup_results\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# import results/full_model.csv as dataframe\n",
    "df = pd.read_csv(os.path.join('results', 'full_model.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">wikitext2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p16</th>\n",
       "      <td>7.20</td>\n",
       "      <td>6.05</td>\n",
       "      <td>5.19</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.78</td>\n",
       "      <td>10.15</td>\n",
       "      <td>11.33</td>\n",
       "      <td>10.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p32</th>\n",
       "      <td>7.86</td>\n",
       "      <td>6.42</td>\n",
       "      <td>5.62</td>\n",
       "      <td>8.36</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.24</td>\n",
       "      <td>11.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p64</th>\n",
       "      <td>8.38</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.02</td>\n",
       "      <td>8.82</td>\n",
       "      <td>10.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p16</th>\n",
       "      <td>7.18</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.20</td>\n",
       "      <td>7.80</td>\n",
       "      <td>10.78</td>\n",
       "      <td>10.16</td>\n",
       "      <td>11.32</td>\n",
       "      <td>10.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p32</th>\n",
       "      <td>7.82</td>\n",
       "      <td>6.46</td>\n",
       "      <td>5.62</td>\n",
       "      <td>8.34</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.24</td>\n",
       "      <td>11.38</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p64</th>\n",
       "      <td>8.39</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.04</td>\n",
       "      <td>8.82</td>\n",
       "      <td>10.97</td>\n",
       "      <td>10.35</td>\n",
       "      <td>11.40</td>\n",
       "      <td>10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p16</th>\n",
       "      <td>7.79</td>\n",
       "      <td>6.46</td>\n",
       "      <td>5.63</td>\n",
       "      <td>11.75</td>\n",
       "      <td>10.96</td>\n",
       "      <td>10.29</td>\n",
       "      <td>11.84</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p16_wg128</th>\n",
       "      <td>7.51</td>\n",
       "      <td>6.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.02</td>\n",
       "      <td>10.19</td>\n",
       "      <td>11.62</td>\n",
       "      <td>10.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p32</th>\n",
       "      <td>8.63</td>\n",
       "      <td>6.98</td>\n",
       "      <td>6.12</td>\n",
       "      <td>12.58</td>\n",
       "      <td>11.08</td>\n",
       "      <td>10.40</td>\n",
       "      <td>11.89</td>\n",
       "      <td>10.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p32_wg128</th>\n",
       "      <td>8.25</td>\n",
       "      <td>6.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11.11</td>\n",
       "      <td>10.32</td>\n",
       "      <td>11.66</td>\n",
       "      <td>10.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_arc_challenge</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_arc_easy</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_boolq</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_hellaswag</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_piqa</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_winogrande</th>\n",
       "      <td>9.42</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.28</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_wg128</th>\n",
       "      <td>9.30</td>\n",
       "      <td>7.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.83</td>\n",
       "      <td>11.31</td>\n",
       "      <td>10.30</td>\n",
       "      <td>11.71</td>\n",
       "      <td>10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p16</th>\n",
       "      <td>7.88</td>\n",
       "      <td>6.52</td>\n",
       "      <td>5.67</td>\n",
       "      <td>11.36</td>\n",
       "      <td>12.84</td>\n",
       "      <td>10.77</td>\n",
       "      <td>14.02</td>\n",
       "      <td>194.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p32</th>\n",
       "      <td>8.66</td>\n",
       "      <td>7.10</td>\n",
       "      <td>6.18</td>\n",
       "      <td>12.89</td>\n",
       "      <td>12.95</td>\n",
       "      <td>10.85</td>\n",
       "      <td>14.08</td>\n",
       "      <td>157.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p64</th>\n",
       "      <td>9.48</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.69</td>\n",
       "      <td>14.47</td>\n",
       "      <td>13.26</td>\n",
       "      <td>10.99</td>\n",
       "      <td>14.11</td>\n",
       "      <td>117.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               wikitext2                 \\\n",
       "model                                     llama-13b-meta llama-30b-meta   \n",
       "experiment                                                                \n",
       "all_W16A4O12_ol1p16                                 7.20           6.05   \n",
       "all_W16A4O12_ol1p32                                 7.86           6.42   \n",
       "all_W16A4O12_ol1p64                                 8.38           6.86   \n",
       "all_W16A4O16_ol1p16                                 7.18           6.04   \n",
       "all_W16A4O16_ol1p32                                 7.82           6.46   \n",
       "all_W16A4O16_ol1p64                                 8.39           6.89   \n",
       "all_W4A4O12_ol1p16                                  7.79           6.46   \n",
       "all_W4A4O12_ol1p16_wg128                            7.51           6.24   \n",
       "all_W4A4O12_ol1p32                                  8.63           6.98   \n",
       "all_W4A4O12_ol1p32_wg128                            8.25           6.71   \n",
       "all_W4A4O12_ol1p64                                  9.42           7.39   \n",
       "all_W4A4O12_ol1p64_accuracy                          NaN            NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge           9.42           7.39   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy                9.42           7.39   \n",
       "all_W4A4O12_ol1p64_accuracy_boolq                   9.42           7.39   \n",
       "all_W4A4O12_ol1p64_accuracy_hellaswag                NaN            NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_piqa                    9.42           7.39   \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande              9.42           7.39   \n",
       "all_W4A4O12_ol1p64_wg128                            9.30           7.40   \n",
       "all_W4A4O8_ol1p16                                   7.88           6.52   \n",
       "all_W4A4O8_ol1p32                                   8.66           7.10   \n",
       "all_W4A4O8_ol1p64                                   9.48           7.51   \n",
       "\n",
       "                                                                        \\\n",
       "model                                     llama-65b-meta llama-7b-meta   \n",
       "experiment                                                               \n",
       "all_W16A4O12_ol1p16                                 5.19          7.79   \n",
       "all_W16A4O12_ol1p32                                 5.62          8.36   \n",
       "all_W16A4O12_ol1p64                                 6.02          8.82   \n",
       "all_W16A4O16_ol1p16                                 5.20          7.80   \n",
       "all_W16A4O16_ol1p32                                 5.62          8.34   \n",
       "all_W16A4O16_ol1p64                                 6.04          8.82   \n",
       "all_W4A4O12_ol1p16                                  5.63         11.75   \n",
       "all_W4A4O12_ol1p16_wg128                             NaN          9.67   \n",
       "all_W4A4O12_ol1p32                                  6.12         12.58   \n",
       "all_W4A4O12_ol1p32_wg128                             NaN         10.65   \n",
       "all_W4A4O12_ol1p64                                  6.59         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy                          NaN           NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge           6.59         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy                6.59         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy_boolq                   6.59         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy_hellaswag                NaN         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy_piqa                    6.59         14.49   \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande              6.59         14.49   \n",
       "all_W4A4O12_ol1p64_wg128                             NaN         12.83   \n",
       "all_W4A4O8_ol1p16                                   5.67         11.36   \n",
       "all_W4A4O8_ol1p32                                   6.18         12.89   \n",
       "all_W4A4O8_ol1p64                                   6.69         14.47   \n",
       "\n",
       "                                                                            \n",
       "model                                     opt-13b opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                                  \n",
       "all_W16A4O12_ol1p16                         10.78   10.15    11.33   10.06  \n",
       "all_W16A4O12_ol1p32                         10.93   10.24    11.38     NaN  \n",
       "all_W16A4O12_ol1p64                         10.98     NaN    11.40     NaN  \n",
       "all_W16A4O16_ol1p16                         10.78   10.16    11.32   10.03  \n",
       "all_W16A4O16_ol1p32                         10.93   10.24    11.38   10.20  \n",
       "all_W16A4O16_ol1p64                         10.97   10.35    11.40   10.25  \n",
       "all_W4A4O12_ol1p16                          10.96   10.29    11.84   10.38  \n",
       "all_W4A4O12_ol1p16_wg128                    11.02   10.19    11.62   10.18  \n",
       "all_W4A4O12_ol1p32                          11.08   10.40    11.89   10.44  \n",
       "all_W4A4O12_ol1p32_wg128                    11.11   10.32    11.66   10.28  \n",
       "all_W4A4O12_ol1p64                          11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy                   NaN     NaN      NaN   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge   11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy        11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy_boolq           11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy_hellaswag       11.28     NaN    11.90     NaN  \n",
       "all_W4A4O12_ol1p64_accuracy_piqa            11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande      11.28   10.50    11.90   10.47  \n",
       "all_W4A4O12_ol1p64_wg128                    11.31   10.30    11.71   10.25  \n",
       "all_W4A4O8_ol1p16                           12.84   10.77    14.02  194.24  \n",
       "all_W4A4O8_ol1p32                           12.95   10.85    14.08  157.20  \n",
       "all_W4A4O8_ol1p64                           13.26   10.99    14.11  117.36  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['wikitext2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">c4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p16</th>\n",
       "      <td>9.08</td>\n",
       "      <td>7.86</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.76</td>\n",
       "      <td>11.72</td>\n",
       "      <td>11.21</td>\n",
       "      <td>12.19</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p32</th>\n",
       "      <td>9.69</td>\n",
       "      <td>8.24</td>\n",
       "      <td>7.93</td>\n",
       "      <td>10.32</td>\n",
       "      <td>11.85</td>\n",
       "      <td>11.30</td>\n",
       "      <td>12.24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O12_ol1p64</th>\n",
       "      <td>10.23</td>\n",
       "      <td>8.61</td>\n",
       "      <td>8.44</td>\n",
       "      <td>10.74</td>\n",
       "      <td>11.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p16</th>\n",
       "      <td>9.11</td>\n",
       "      <td>7.86</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.72</td>\n",
       "      <td>11.21</td>\n",
       "      <td>12.20</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p32</th>\n",
       "      <td>9.71</td>\n",
       "      <td>8.24</td>\n",
       "      <td>7.94</td>\n",
       "      <td>10.31</td>\n",
       "      <td>11.86</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.23</td>\n",
       "      <td>10.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W16A4O16_ol1p64</th>\n",
       "      <td>10.24</td>\n",
       "      <td>8.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>10.77</td>\n",
       "      <td>11.89</td>\n",
       "      <td>11.37</td>\n",
       "      <td>12.26</td>\n",
       "      <td>10.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p16</th>\n",
       "      <td>10.27</td>\n",
       "      <td>8.47</td>\n",
       "      <td>7.94</td>\n",
       "      <td>14.33</td>\n",
       "      <td>12.01</td>\n",
       "      <td>11.41</td>\n",
       "      <td>12.88</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p16_wg128</th>\n",
       "      <td>9.67</td>\n",
       "      <td>8.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.69</td>\n",
       "      <td>11.92</td>\n",
       "      <td>11.32</td>\n",
       "      <td>12.61</td>\n",
       "      <td>10.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p32</th>\n",
       "      <td>11.20</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.59</td>\n",
       "      <td>15.89</td>\n",
       "      <td>12.12</td>\n",
       "      <td>11.53</td>\n",
       "      <td>12.94</td>\n",
       "      <td>11.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p32_wg128</th>\n",
       "      <td>10.51</td>\n",
       "      <td>8.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.96</td>\n",
       "      <td>12.02</td>\n",
       "      <td>11.42</td>\n",
       "      <td>12.65</td>\n",
       "      <td>10.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64</th>\n",
       "      <td>12.24</td>\n",
       "      <td>9.40</td>\n",
       "      <td>9.26</td>\n",
       "      <td>16.57</td>\n",
       "      <td>12.24</td>\n",
       "      <td>11.62</td>\n",
       "      <td>12.96</td>\n",
       "      <td>11.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_wg128</th>\n",
       "      <td>12.05</td>\n",
       "      <td>9.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.83</td>\n",
       "      <td>12.21</td>\n",
       "      <td>11.42</td>\n",
       "      <td>12.72</td>\n",
       "      <td>10.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p16</th>\n",
       "      <td>10.43</td>\n",
       "      <td>8.50</td>\n",
       "      <td>7.99</td>\n",
       "      <td>14.54</td>\n",
       "      <td>14.02</td>\n",
       "      <td>12.19</td>\n",
       "      <td>14.78</td>\n",
       "      <td>281.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p32</th>\n",
       "      <td>11.35</td>\n",
       "      <td>8.95</td>\n",
       "      <td>8.67</td>\n",
       "      <td>16.04</td>\n",
       "      <td>14.07</td>\n",
       "      <td>12.34</td>\n",
       "      <td>15.01</td>\n",
       "      <td>217.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O8_ol1p64</th>\n",
       "      <td>12.42</td>\n",
       "      <td>9.42</td>\n",
       "      <td>9.41</td>\n",
       "      <td>16.80</td>\n",
       "      <td>14.31</td>\n",
       "      <td>12.44</td>\n",
       "      <td>14.97</td>\n",
       "      <td>208.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     c4                                \\\n",
       "model                    llama-13b-meta llama-30b-meta llama-65b-meta   \n",
       "experiment                                                              \n",
       "all_W16A4O12_ol1p16                9.08           7.86           7.41   \n",
       "all_W16A4O12_ol1p32                9.69           8.24           7.93   \n",
       "all_W16A4O12_ol1p64               10.23           8.61           8.44   \n",
       "all_W16A4O16_ol1p16                9.11           7.86           7.41   \n",
       "all_W16A4O16_ol1p32                9.71           8.24           7.94   \n",
       "all_W16A4O16_ol1p64               10.24           8.62           8.47   \n",
       "all_W4A4O12_ol1p16                10.27           8.47           7.94   \n",
       "all_W4A4O12_ol1p16_wg128           9.67           8.09            NaN   \n",
       "all_W4A4O12_ol1p32                11.20           8.92           8.59   \n",
       "all_W4A4O12_ol1p32_wg128          10.51           8.49            NaN   \n",
       "all_W4A4O12_ol1p64                12.24           9.40           9.26   \n",
       "all_W4A4O12_ol1p64_wg128          12.05           9.01            NaN   \n",
       "all_W4A4O8_ol1p16                 10.43           8.50           7.99   \n",
       "all_W4A4O8_ol1p32                 11.35           8.95           8.67   \n",
       "all_W4A4O8_ol1p64                 12.42           9.42           9.41   \n",
       "\n",
       "                                                                         \n",
       "model                    llama-7b-meta opt-13b opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                               \n",
       "all_W16A4O12_ol1p16               9.76   11.72   11.21    12.19   10.64  \n",
       "all_W16A4O12_ol1p32              10.32   11.85   11.30    12.24     NaN  \n",
       "all_W16A4O12_ol1p64              10.74   11.89     NaN    12.28     NaN  \n",
       "all_W16A4O16_ol1p16               9.77   11.72   11.21    12.20   10.64  \n",
       "all_W16A4O16_ol1p32              10.31   11.86   11.31    12.23   10.70  \n",
       "all_W16A4O16_ol1p64              10.77   11.89   11.37    12.26   10.73  \n",
       "all_W4A4O12_ol1p16               14.33   12.01   11.41    12.88   10.99  \n",
       "all_W4A4O12_ol1p16_wg128         12.69   11.92   11.32    12.61   10.79  \n",
       "all_W4A4O12_ol1p32               15.89   12.12   11.53    12.94   11.01  \n",
       "all_W4A4O12_ol1p32_wg128         13.96   12.02   11.42    12.65   10.82  \n",
       "all_W4A4O12_ol1p64               16.57   12.24   11.62    12.96   11.04  \n",
       "all_W4A4O12_ol1p64_wg128         15.83   12.21   11.42    12.72   10.84  \n",
       "all_W4A4O8_ol1p16                14.54   14.02   12.19    14.78  281.18  \n",
       "all_W4A4O8_ol1p32                16.04   14.07   12.34    15.01  217.29  \n",
       "all_W4A4O8_ol1p64                16.80   14.31   12.44    14.97  208.73  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['c4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">piqa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_piqa</th>\n",
       "      <td>0.6806</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>0.7524</td>\n",
       "      <td>0.6583</td>\n",
       "      <td>0.7437</td>\n",
       "      <td>0.7492</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>0.7671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           piqa                                \\\n",
       "model                            llama-13b-meta llama-30b-meta llama-65b-meta   \n",
       "experiment                                                                      \n",
       "all_W4A4O12_ol1p64_accuracy                 NaN            NaN            NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_piqa         0.6806         0.7329         0.7524   \n",
       "\n",
       "                                                                         \\\n",
       "model                            llama-7b-meta opt-13b opt-30b opt-6.7b   \n",
       "experiment                                                                \n",
       "all_W4A4O12_ol1p64_accuracy                NaN     NaN     NaN   0.7372   \n",
       "all_W4A4O12_ol1p64_accuracy_piqa        0.6583  0.7437  0.7492   0.7372   \n",
       "\n",
       "                                          \n",
       "model                            opt-66b  \n",
       "experiment                                \n",
       "all_W4A4O12_ol1p64_accuracy          NaN  \n",
       "all_W4A4O12_ol1p64_accuracy_piqa  0.7671  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['piqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">arc_easy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_arc_easy</th>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.4562</td>\n",
       "      <td>0.6469</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.6338</td>\n",
       "      <td>0.6814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           arc_easy                 \\\n",
       "model                                llama-13b-meta llama-30b-meta   \n",
       "experiment                                                           \n",
       "all_W4A4O12_ol1p64_accuracy                     NaN            NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy         0.5349         0.6048   \n",
       "\n",
       "                                                                           \\\n",
       "model                                llama-65b-meta llama-7b-meta opt-13b   \n",
       "experiment                                                                  \n",
       "all_W4A4O12_ol1p64_accuracy                     NaN        0.4562     NaN   \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy         0.6456        0.4562  0.6469   \n",
       "\n",
       "                                                               \n",
       "model                                opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                     \n",
       "all_W4A4O12_ol1p64_accuracy              NaN      NaN     NaN  \n",
       "all_W4A4O12_ol1p64_accuracy_arc_easy  0.6515   0.6338  0.6814  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['arc_easy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">arc_challenge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_arc_challenge</th>\n",
       "      <td>0.2935</td>\n",
       "      <td>0.3746</td>\n",
       "      <td>0.3882</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.3396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           arc_challenge                 \\\n",
       "model                                     llama-13b-meta llama-30b-meta   \n",
       "experiment                                                                \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge         0.2935         0.3746   \n",
       "\n",
       "                                                                        \\\n",
       "model                                     llama-65b-meta llama-7b-meta   \n",
       "experiment                                                               \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge         0.3882        0.2841   \n",
       "\n",
       "                                                                            \n",
       "model                                     opt-13b opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                                  \n",
       "all_W4A4O12_ol1p64_accuracy_arc_challenge  0.3097  0.3131   0.2952  0.3396  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['arc_challenge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">boolq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_boolq</th>\n",
       "      <td>0.6327</td>\n",
       "      <td>0.6755</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.6098</td>\n",
       "      <td>0.5917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           boolq                 \\\n",
       "model                             llama-13b-meta llama-30b-meta   \n",
       "experiment                                                        \n",
       "all_W4A4O12_ol1p64_accuracy_boolq         0.6327         0.6755   \n",
       "\n",
       "                                                                        \\\n",
       "model                             llama-65b-meta llama-7b-meta opt-13b   \n",
       "experiment                                                               \n",
       "all_W4A4O12_ol1p64_accuracy_boolq         0.6948         0.556  0.6645   \n",
       "\n",
       "                                                            \n",
       "model                             opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                  \n",
       "all_W4A4O12_ol1p64_accuracy_boolq  0.6645   0.6098  0.5917  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['boolq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">winogrande</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>llama-13b-meta</th>\n",
       "      <th>llama-30b-meta</th>\n",
       "      <th>llama-65b-meta</th>\n",
       "      <th>llama-7b-meta</th>\n",
       "      <th>opt-13b</th>\n",
       "      <th>opt-30b</th>\n",
       "      <th>opt-6.7b</th>\n",
       "      <th>opt-66b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_W4A4O12_ol1p64_accuracy_winogrande</th>\n",
       "      <td>0.5343</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6511</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.6788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           winogrande                 \\\n",
       "model                                  llama-13b-meta llama-30b-meta   \n",
       "experiment                                                             \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande         0.5343         0.5596   \n",
       "\n",
       "                                                                             \\\n",
       "model                                  llama-65b-meta llama-7b-meta opt-13b   \n",
       "experiment                                                                    \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande         0.6077        0.5233  0.6393   \n",
       "\n",
       "                                                                 \n",
       "model                                  opt-30b opt-6.7b opt-66b  \n",
       "experiment                                                       \n",
       "all_W4A4O12_ol1p64_accuracy_winogrande  0.6511   0.6156  0.6788  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['experiment'], columns=['model'], values=['winogrande'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
